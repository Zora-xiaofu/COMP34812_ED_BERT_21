{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Lp0ZG4L3l4xT",
      "metadata": {
        "id": "Lp0ZG4L3l4xT"
      },
      "source": [
        "# Inference Demo BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e75c89a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e75c89a",
        "outputId": "99363268-2dec-456f-8db1-e6a6b4a41923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Inference Demo Code - Solution C - BERT\n",
        "\n",
        "# Please install the following libraries before running the code\n",
        "# !pip install torch\n",
        "# !pip install transformers\n",
        "# !pip install pandas\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "MAX_LEN = 128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "  text = re.sub(r\"can't\\b\", \"cannot\", text)\n",
        "  text = re.sub(r\"won't\\b\", \"will not\", text)\n",
        "  text = re.sub(r\"n't\\b\", \" not\", text)\n",
        "  text = re.sub(r\"'re\\b\", \" are\", text)\n",
        "  text = re.sub(r\"'m\\b\", \" am\", text)\n",
        "  text = re.sub(r\"'ve\\b\", \" have\", text)\n",
        "  text = re.sub(r\"'ll\\b\", \" will\", text)\n",
        "  text = re.sub(r\"'d\\b\", \" would\", text)\n",
        "  text = re.sub(r\"\\b(he|she|it|that|what|who|there|where|why|when)'s\\b\", r\"\\1 is\", text, flags=re.IGNORECASE)\n",
        "  text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"[URL]\", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "  return text\n",
        "\n",
        "# Custom Dataset class\n",
        "class EDDataset(Dataset):\n",
        "  def __init__(self, texts, labels=None, tokenizer=None):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer or BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    encoding = self.tokenizer(\n",
        "        self.texts[index],\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    item = {\n",
        "        'input_ids': encoding['input_ids'].squeeze(0),\n",
        "        'attention_mask': encoding['attention_mask'].squeeze(0)\n",
        "    }\n",
        "    if self.labels is not None:\n",
        "      item['labels'] = torch.tensor(self.labels[index], dtype=torch.long)\n",
        "    return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7AjP-Aj9mZBI",
      "metadata": {
        "id": "7AjP-Aj9mZBI"
      },
      "source": [
        "# Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yGP-FW4BNtyN",
      "metadata": {
        "id": "yGP-FW4BNtyN"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device):\n",
        "  model.eval()\n",
        "  all_logits = []\n",
        "  # Turn off gradient tracking to speed up prediction and save memory\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      logits = outputs.logits\n",
        "      all_logits.append(logits)\n",
        "  return torch.cat(all_logits, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iMTxlwb1mezr",
      "metadata": {
        "id": "iMTxlwb1mezr"
      },
      "source": [
        "# Run Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6566d03f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6566d03f",
        "outputId": "21c91da9-90b8-4274-b157-35b1618c50a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /content/drive/My Drive/nlu-lab/bert_model/Group_21_C.csv\n",
            "   prediction\n",
            "0           1\n",
            "1           0\n",
            "2           0\n",
            "3           0\n",
            "4           0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  model_dir = '/content/drive/My Drive/nlu-lab/bert_model'\n",
        "  model_path = os.path.join(model_dir, 'best_model_bert.pt')\n",
        "  best_param_path = os.path.join(model_dir, 'best_params_bert.json')\n",
        "\n",
        "  # Load best hyperparameters\n",
        "  with open(best_param_path, 'r') as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "  # Load the fine-tuned model\n",
        "  model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  # Load test data (e.g., dev-nonlabel.csv)\n",
        "  test_file_path = '/content/drive/My Drive/nlu-lab/test.csv'\n",
        "  test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "  # Clean and tokenize input\n",
        "  test_df['combined_text'] = (test_df['Claim'] + \" \" + test_df['Evidence']).apply(clean_text)\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  test_dataset = EDDataset(test_df['combined_text'].tolist(), tokenizer=tokenizer)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "  # Perform prediction\n",
        "  test_logits = predict(model, test_dataloader, device)\n",
        "  test_pred = torch.argmax(test_logits, dim=1)\n",
        "\n",
        "  # Save predictions to CSV\n",
        "  output_path = os.path.join(model_dir, 'Group_21_C.csv')\n",
        "  results = pd.DataFrame({'prediction': test_pred.cpu().numpy()})\n",
        "  results.to_csv(output_path, index=False)\n",
        "  print(f\"Results saved to: {output_path}\")\n",
        "  print(results.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
